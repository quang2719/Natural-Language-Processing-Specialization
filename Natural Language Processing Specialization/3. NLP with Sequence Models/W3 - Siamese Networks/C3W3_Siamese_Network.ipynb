{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Siamese model: Ungraded Lecture Notebook\n",
    "\n",
    "In this notebook you will learn how to create a siamese model in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 07:36:55.042932: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-16 07:36:55.098227: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-16 07:36:55.099246: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 07:36:56.267516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow import math\n",
    "import numpy\n",
    "\n",
    "# Setting random seeds\n",
    "numpy.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Siamese` model you will first need to create a LSTM model. For this you can stack layers using the`Sequential` model. To retrieve the output of both branches of the Siamese model, you can concatenate results using the `Concatenate` layer. You should be familiar with the following layers (notice each layer can be clicked to go to the docs):\n",
    "   - [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) groups a linear stack of layers into a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "   - [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) Maps positive integers into vectors of fixed size. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (called `model_dimension`in the code) is the number of elements in the word embedding. \n",
    "   - [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) The Long Short-Term Memory (LSTM) layer. The number of units should be specified and should match the number of elements in the word embedding. \n",
    "   - [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) Computes global average pooling, which essentially takes the mean across a desired axis. GlobalAveragePooling1D uses one tensor axis to form groups of values and replaces each group with the mean value of that group. \n",
    "   - [`Lambda`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)  Layer with no weights that applies the function f, which should be specified using a lambda syntax. You will use this layer to apply normalization with the function\n",
    "        - `tfmath.l2_normalize(x)`\n",
    "        \n",
    "- [`Concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) Layer that concatenates a list of inputs. This layer will concatenate the normalized outputs of each LSTM into a single output for the model.\n",
    "- [`Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input): it is used to instantiate a Keras tensor.. Remember to set correctly the dimension and type of the input, which are batches of questions. \n",
    "\n",
    "\n",
    "Putting everything together the Siamese model will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, None, 128)            195584    ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, 128)            0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 195584 (764.00 KB)\n",
      "Trainable params: 195584 (764.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "model_dimension = 128\n",
    "\n",
    "# Define the LSTM model\n",
    "LSTM = Sequential()\n",
    "LSTM.add(layers.Embedding(input_dim=vocab_size, output_dim=model_dimension))\n",
    "LSTM.add(layers.LSTM(units=model_dimension, return_sequences = True))\n",
    "LSTM.add(layers.AveragePooling1D())\n",
    "LSTM.add(layers.Lambda(lambda x: math.l2_normalize(x)))\n",
    "\n",
    "input1 = layers.Input((None,))\n",
    "input2 = layers.Input((None,))\n",
    "\n",
    "# Concatenate two LSTMs together\n",
    "conc = layers.Concatenate(axis=1)((LSTM(input1), LSTM(input2)))\n",
    "    \n",
    "\n",
    "# Use the Parallel combinator to create a Siamese model out of the LSTM \n",
    "Siamese = Model(inputs=(input1, input2), outputs=conc)\n",
    "\n",
    "# Print the summary of the model\n",
    "Siamese.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a helper function that prints information for every layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese model:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "========\n",
      "Parallel.sublayers_0: <keras.src.engine.input_layer.InputLayer object at 0x7eabb442b250>\n",
      "\n",
      "========\n",
      "Parallel.sublayers_1: <keras.src.engine.input_layer.InputLayer object at 0x7eabb442ae30>\n",
      "\n",
      "========\n",
      "Parallel.sublayers_2: <keras.src.engine.sequential.Sequential object at 0x7eabb5685cc0>\n",
      "\n",
      "========\n",
      "Parallel.sublayers_3: <keras.src.layers.merging.concatenate.Concatenate object at 0x7eabb4428a30>\n",
      "\n",
      "Detail of LSTM models:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "========\n",
      "Serial.sublayers_0: <keras.src.layers.core.embedding.Embedding object at 0x7eabb5685bd0>\n",
      "\n",
      "========\n",
      "Serial.sublayers_1: <keras.src.layers.rnn.lstm.LSTM object at 0x7eabb5686530>\n",
      "\n",
      "========\n",
      "Serial.sublayers_2: <keras.src.layers.pooling.average_pooling1d.AveragePooling1D object at 0x7eabb442b970>\n",
      "\n",
      "========\n",
      "Serial.sublayers_3: <keras.src.layers.core.lambda_layer.Lambda object at 0x7eabb442b1c0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix):\n",
    "    print(f\"Total layers: {len(model.layers)}\\n\")\n",
    "    for i in range(len(model.layers)):\n",
    "        print('========')\n",
    "        print(f'{layer_prefix}_{i}: {model.layers[i]}\\n')\n",
    "\n",
    "print('Siamese model:\\n')\n",
    "show_layers(Siamese, 'Parallel.sublayers')\n",
    "\n",
    "print('Detail of LSTM models:\\n')\n",
    "show_layers(LSTM, 'Serial.sublayers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the parameters defined before the Siamese model and see how it changes!\n",
    "\n",
    "You will actually train this model in this week's assignment. For now you should be more familiarized with creating Siamese models using TensorFlow. **Keep it up!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
